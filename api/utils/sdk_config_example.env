## ==============================
## 		Denodo-Gen-AI-SDK
##	    configuration parameters
## 
##   Within this file you will need to set all the necessary parameters for configuring the AI SDK.
##   Carefully go through all the sections to ensure the AI SDK works properly with your environment.
##  
##   You will need to review the following 6 sections
##   1- API CONFIGURATION
##   2- DATA CATALOG CONFIGURATION
##   3- VECTOR STORE CONFIGURATION
##   4- LLM/EMBEDDINGS CONFIGURATION
##   5- LLM Provider configuration
## ==============================

AI_SDK_VER = 0.4

## ==============================
## 1.
## API CONFIGURATION
## Establish the host and port the API should listen from.
## ==============================

AI_SDK_HOST = 0.0.0.0
AI_SDK_PORT = 8008

## If you want to activate HTTPS, write down the key and certificate path here.

#AI_SDK_SSL_CERT = 
#AI_SDK_SSL_KEY = 

## ==============================
## 2.
## DATA CATALOG CONFIGURATION
## Parameters that determine the connection to the Data Catalog to retrieve the metadata and execute queries.
## VDB_NAMES is a list of VDBs, separated by commas, that you want to expose to the AI SDK.
## The DATA_CATALOG_METADATA_USER is the user that will vectorize all the metadata in DC.
## Example DATA_CATALOG_URL: http://localhost:9090/denodo-data-catalog/
## ==============================

VDB_NAMES = samples_bank
DATA_CATALOG_URL = http://localhost:9090/denodo-data-catalog/

## DATA CATALOG auth supports HTTP Basic with Username, Password or Oauth with access token
## HTTP Basic with admin/admin is configured by default

# DATA_CATALOG_AUTH_TYPE can take two values: http_basic or oauth
DATA_CATALOG_AUTH_TYPE = http_basic
DATA_CATALOG_METADATA_USER = admin
DATA_CATALOG_METADATA_PWD = admin

## DATA_CATALOG_METADATA_OAUTH can be used to set the Oauth access token
#DATA_CATALOG_METADATA_OAUTH =

## DATA_CATALOG_SERVER_ID defines the ID of the VDP connected to the Data Catalog, it defaults to 1 if not set. 
## Only modify this property if you have multiple VDP's connected to your Data Catalog

DATA_CATALOG_SERVER_ID = 1

## USER PERMISSIONS activates user permissions. Needs Denodo 9.0.2 hotfix (20240824) or Denodo 8 hotfix (20240814).
## When switching from USER_PERMISSIONS 0 to 1, make sure to overwrite the contents of the vector store.

USER_PERMISSIONS = 0

# If using SSL, you can set to 0 to not verify or you can include the path of the certificates to verify.

DATA_CATALOG_VERIFY_SSL = 0

##==============================
## 3.
## VECTOR STORE CONFIGURATION
## Select the vector store provider and then the specific configuration for that vector store
## Available: Chroma, PGVector, OpenSearch
##==============================

## VECTOR_STORE is the provider of the vector store

VECTOR_STORE = chroma

##==============================
## PGVector
##==============================

# The full connection string to the PGVector with username and password. For example: postgresql+psycopg://langchain:langchain@localhost:6024/langchain
# Must include postgresql+psycopg at the beginning. After that it's user:pwd@host:port/db

PGVECTOR_CONNECTION_STRING = 

##==============================
## OpenSearch
##==============================

# The URL of the OpenSearch instance. Default: http://localhost:9200
OPENSEARCH_URL =
OPENSEARCH_USERNAME =
OPENSEARCH_PASSWORD = 

## ==============================
## 4.
## LLM/EMBEDDINGS CONFIGURATION
## Fill in the corresponding parameters below.
## Compatible providers: Bedrock, OpenAI, AzureOpenAI, Google, Anthropic, Ollama
## ==============================

## CHAT_PROVIDER defines which LLM you will be using for simple conversation tasks.

CHAT_PROVIDER = OpenAI

## CHAT_MODEL defines the specific model ID that will be using for chat. 
##  Examples: anthropic.claude-3-5-sonnet-20240620-v1:0, Gemini-1.5-flash, meta.llama2-13b-chat-v1, etc..

CHAT_MODEL = gpt-4o

## SQL_GENERATION_PROVIDER defines the specific LLM provider you will be using for the SQL generation. 

SQL_GENERATION_PROVIDER = OpenAI

## SQL_GENERATION_MODEL defines the specific model ID that will be using for the SQL generation.
## Examples: anthropic.claude-3-5-sonnet-20240620-v1:0, Gemini-1.5-flash, meta.llama2-13b-chat-v1, etc..

SQL_GENERATION_MODEL = gpt-4o

## EMBEDDINGS_PROVIDER defines the specific provider you will be using for the embeddings.

EMBEDDINGS_PROVIDER = OpenAI

## EMBEDDINGS_MODEL defines the specific model ID that will be using for the embeddings.
## Examples: text-embedding-3-large, amazon.titan-embed-text-v1, text-embedding-004, etc..

EMBEDDINGS_MODEL = text-embedding-3-large

## ==============================
## 5.
## LLM PROVIDER CONFIGURATION
## Fill in the corresponding parameters for your specific LLM provider.
## ==============================

##==============================
## Ollama
## There is no specific configuration for Ollama, but:
##      - You must have the model already installed first through Ollama
##      - You must use the same model ID in CHAT_MODEL and SQL_GENERATION_MODEL as the one you use in Ollama
## There's no need to execute 'ollama run <model-id>' to use it in the AI SDK.
##==============================

##==============================
## OpenAI
## If you want to have two different OpenAI-API compatible providers, please check the user manual.
##==============================

## OPENAI_API_KEY defines the API key for your OpenAI account.

OPENAI_API_KEY = 

## OpenAI base url can be set OPTIONALLY if using a OpenAI-compatible provider different from OpenAI.

#OPENAI_BASE_URL = 

## OpenAI proxy to use. Set as http://{user}:{pwd}@{host}:{port} format

#OPENAI_PROXY = 

## OpenAI organization ID. If not set it will use the default.
#OPENAI_ORG_ID = 

## You can set the dimensions (size of the vector object) with this variable for the embeddings model.
## It is recommended to leave it to the model's default dimension size.

#OPENAI_EMBEDDINGS_DIMENSIONS = 

##==============================
## AzureOpenAI
## Please set the deployment name in the CHAT_MODEL/SQL_GENERATION_MODEL variables.
## The model (deployment) name used will be appended to the Azure OpenAI endpoint, like this:
## /deployments/{CHAT_MODEL}
##==============================

## AZURE_OPENAI_ENDPOINT and AZURE_API_VERSION define the connection string and version to your AzureOpenAI instance.
## AZURE_OPENAI_ENDPOINT refers to the everything until the azure.com domain. For example: https://example-resource.openai.azure.com/
## The AI SDK will automatically append /openai/deployments/{model_name}/chat/completions... to the endpoint.

AZURE_OPENAI_ENDPOINT = 
AZURE_API_VERSION = 

##AZURE_OPENAI_API_KEY defines the API key for your OpenAI account.

AZURE_OPENAI_API_KEY = 

## AzureOpenAI proxy to use. Set as http://{user}:{pwd}@{host}:{port} format

#AZURE_OPENAI_PROXY = 

## You can set the dimensions (size of the vector object) with this variable for the embeddings model.
## It is recommended to leave it to the model's default dimension size.

#AZUREOPENAI_EMBEDDINGS_DIMENSIONS = 

##==============================
## Google
##==============================

## GOOGLE_APPLICATION_CREDENTIALS defines the path to the JSON storing your Google Application Credentials

GOOGLE_APPLICATION_CREDENTIALS = 

##==============================
## Groq
##==============================

## GROQ_API_KEY defines the API key for your OpenAI account.

GROQ_API_KEY = 

##==============================
## Bedrock
##==============================

## AWS_REGION, AWS_PROFILE_NAME, AWS_ROLE_ARN, AWS_SECRET_ACCESS_KEY, and AWS_ACCESS_KEY_ID define the connection parameters to your AWS Bedrock instance.
## If using EC2 with an IAM profile, you only need to set AWS_REGION to select the region you want to deploy Bedrock in.
## This is relevant because different regions have different models/costs/latencies and it is a required parameter by AWS when making a call.

AWS_REGION = 
AWS_PROFILE_NAME = 
AWS_ROLE_ARN = 
AWS_ACCESS_KEY_ID = 
AWS_SECRET_ACCESS_KEY = 

##==============================
## Mistral
##==============================

MISTRAL_API_KEY = 

##==============================
## NVIDIA NIM
##==============================

NVIDIA_API_KEY = 

# If self-hosting NVIDIA NIM, set the base url here, like "http://localhost:8000/v1"
#NVIDIA_BASE_URL = 

## ==============================
## 6.
## Extra parameters
## ==============================

# You can use CUSTOM_INSTRUCTIONS to supply the LLM with additional data it may need to correctly resolve your queries.
# This should be used as an complement to the metadata stored in Denodo, not as a substitute.
#CUSTOM_INSTRUCTIONS = 