## ==============================
##      Chatbot Configuration
##
##   This file contains the necessary parameters for configuring the chatbot.
##   Please review all sections carefully to ensure proper functionality.
##
##   Sections:
##   1- CHATBOT CONFIGURATION
##   2- LLM CONFIGURATION
##   3- VECTOR STORE CONFIGURATION
##   4- AI SDK CONFIGURATION
## ==============================

AI_SDK_VER = 0.4

## ==============================
## 1. CHATBOT CONFIGURATION
## Set the host, port, and other general chatbot settings.
## ==============================

CHATBOT_HOST = 0.0.0.0
CHATBOT_PORT = 9992
CHATBOT_OVERWRITE_ON_LOAD = 1
CHATBOT_EXAMPLES_PER_TABLE = 3

## If you want to activate HTTPS, write down the key and certificate path here.
#CHATBOT_SSL_CERT = cert.pem 
#CHATBOT_SSL_KEY = key.pem

## ==============================
## 2. LLM CONFIGURATION
## Specify the LLM provider, model, and API key for both chat and embeddings.
## ==============================

CHATBOT_LLM_PROVIDER = OpenAI
CHATBOT_LLM_MODEL = gpt-4
CHATBOT_EMBEDDINGS_PROVIDER = OpenAI
CHATBOT_EMBEDDINGS_MODEL = text-embedding-3-large

##==============================
## Ollama
## There is no specific configuration for Ollama, but:
##      - You must have the model already installed first through Ollama
##      - You must use the same model ID in CHAT_MODEL and SQL_GENERATION_MODEL as the one you use in Ollama
## There's no need to execute 'ollama run <model-id>' before launching the chatbot.
##==============================

##==============================
## OpenAI
## If you want to have two different OpenAI-API compatible providers, please check the user manual.
##==============================

## OPENAI_API_KEY defines the API key for your OpenAI account.

OPENAI_API_KEY = 

## OpenAI base url can be set OPTIONALLY if using a OpenAI-compatible provider different from OpenAI.

#OPENAI_BASE_URL = 

## OpenAI proxy to use. Set as http://{user}:{pwd}@{host}:{port} format

#OPENAI_PROXY = 

## OpenAI organization ID. If not set it will use the default.
#OPENAI_ORG_ID = 

## You can set the dimensions (size of the vector object) with this variable for the embeddings model.
## It is recommended to leave it to the model's default dimension size.

#OPENAI_EMBEDDINGS_DIMENSIONS = 

##==============================
## AzureOpenAI
## Please set the deployment name in the CHAT_MODEL/SQL_GENERATION_MODEL variables.
## The model (deployment) name used will be appended to the Azure OpenAI endpoint, like this:
## /deployments/{CHAT_MODEL}
##==============================

## AZURE_OPENAI_ENDPOINT and AZURE_API_VERSION define the connection string and version to your AzureOpenAI instance.
## AZURE_OPENAI_ENDPOINT refers to the everything until the azure.com domain. For example: https://example-resource.openai.azure.com/
## The AI SDK will automatically append /openai/deployments/{model_name}/chat/completions... to the endpoint.

AZURE_OPENAI_ENDPOINT = 
AZURE_API_VERSION = 

##AZURE_OPENAI_API_KEY defines the API key for your OpenAI account.

AZURE_OPENAI_API_KEY = 

## AzureOpenAI proxy to use. Set as http://{user}:{pwd}@{host}:{port} format

#AZURE_OPENAI_PROXY = 

## You can set the dimensions (size of the vector object) with this variable for the embeddings model.
## It is recommended to leave it to the model's default dimension size.

#AZUREOPENAI_EMBEDDINGS_DIMENSIONS = 

##==============================
## Google
##==============================

## GOOGLE_APPLICATION_CREDENTIALS defines the path to the JSON storing your Google Application Credentials

GOOGLE_APPLICATION_CREDENTIALS = 

##==============================
## Groq
##==============================

## GROQ_API_KEY defines the API key for your OpenAI account.

GROQ_API_KEY = 

##==============================
## Bedrock
##==============================

## AWS_REGION, AWS_PROFILE_NAME, AWS_ROLE_ARN, AWS_SECRET_ACCESS_KEY, and AWS_ACCESS_KEY_ID define the connection parameters to your AWS Bedrock instance.
## If using EC2 with an IAM profile, you only need to set AWS_REGION to select the region you want to deploy Bedrock in.
## This is relevant because different regions have different models/costs/latencies and it is a required parameter by AWS when making a call.

AWS_REGION = 
AWS_PROFILE_NAME = 
AWS_ROLE_ARN = 
AWS_ACCESS_KEY_ID = 
AWS_SECRET_ACCESS_KEY = 

##==============================
## Mistral
##==============================

MISTRAL_API_KEY = 

##==============================
## NVIDIA NIM
##==============================

NVIDIA_API_KEY = 

# If self-hosting NVIDIA NIM, set the base url here, like "http://localhost:8000/v1"
#NVIDIA_BASE_URL = 

## ==============================
## 3. VECTOR STORE CONFIGURATION
## Specify the vector store provider.
## Available: Chroma, PGVector, OpenSearch
## ==============================

CHATBOT_VECTOR_STORE_PROVIDER = chroma

##==============================
## PGVector
##==============================

# The full connection string to the PGVector with username and password. For example: postgresql+psycopg://langchain:langchain@localhost:6024/langchain
# Must include postgresql+psycopg at the beginning. After that it's user:pwd@host:port/db

PGVECTOR_CONNECTION_STRING = 

##==============================
## OpenSearch
##==============================

# The URL of the OpenSearch instance. Default: http://localhost:9200
OPENSEARCH_URL =
OPENSEARCH_USERNAME =
OPENSEARCH_PASSWORD = 

## ==============================
## 4. AI SDK CONFIGURATION
## Set the connection details for the AI SDK.
## ==============================

AI_SDK_HOST = http://localhost:8008
AI_SDK_USERNAME = admin
AI_SDK_PASSWORD = admin